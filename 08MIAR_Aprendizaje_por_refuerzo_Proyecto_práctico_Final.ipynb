{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUehXgCyIRdq"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "*   Alumno 1: Efrain Casas\n",
    "*   Alumno 2: Manuel Restrepo\n",
    "*   Alumno 3: Ivan Callejas Sandoval\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwpYlnjWJhS9"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU2BPrK2JkP0"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-kixNPiJqTc"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S_YDFwZ-JscI"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(IN_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "I6n7MIefJ21i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos en el directorio: \n",
      "['.ipynb_checkpoints', '08MIAR_Aprendizaje_por_refuerzo_Proyecto_práctico.ipynb', 'Untitled.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "    \n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1ZSL5bpJ560"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UbVRjvHCJ8UF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.17.3 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from gym==0.17.3) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from gym==0.17.3) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from gym==0.17.3) (1.19.5)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from gym==0.17.3) (1.6.0)\n",
      "Requirement already satisfied: future in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Kojoley/atari-py.git\n",
      "  Cloning https://github.com/Kojoley/atari-py.git to c:\\users\\acasa\\appdata\\local\\temp\\pip-req-build-sparmquk\n",
      "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
      "Requirement already satisfied: numpy in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from atari-py==1.2.2) (1.19.5)\n",
      "Building wheels for collected packages: atari-py\n",
      "  Building wheel for atari-py (setup.py): started\n",
      "  Building wheel for atari-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for atari-py\n",
      "Failed to build atari-py\n",
      "Installing collected packages: atari-py\n",
      "  Attempting uninstall: atari-py\n",
      "    Found existing installation: atari-py 0.2.9\n",
      "    Uninstalling atari-py-0.2.9:\n",
      "      Successfully uninstalled atari-py-0.2.9\n",
      "    Running setup.py install for atari-py: started\n",
      "    Running setup.py install for atari-py: finished with status 'error'\n",
      "  Rolling back uninstall of atari-py\n",
      "  Moving to c:\\users\\acasa\\anaconda3\\lib\\site-packages\\atari_py\n",
      "   from C:\\Users\\acasa\\anaconda3\\Lib\\site-packages\\~tari_py\n",
      "  Moving to c:\\users\\acasa\\anaconda3\\lib\\site-packages\\atari_py-0.2.9-py3.9-win-amd64.egg-info\n",
      "   from C:\\Users\\acasa\\anaconda3\\Lib\\site-packages\\~tari_py-0.2.9-py3.9-win-amd64.egg-info\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "  Running command git clone -q https://github.com/Kojoley/atari-py.git 'C:\\Users\\acasa\\AppData\\Local\\Temp\\pip-req-build-sparmquk'\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\acasa\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\acasa\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-sparmquk\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\acasa\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-sparmquk\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\acasa\\AppData\\Local\\Temp\\pip-wheel-mzsfi7jx'\n",
      "       cwd: C:\\Users\\acasa\\AppData\\Local\\Temp\\pip-req-build-sparmquk\\\n",
      "  Complete output (94 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.9\n",
      "  creating build\\lib.win-amd64-3.9\\atari_py\n",
      "  copying atari_py\\ale_python_interface.py -> build\\lib.win-amd64-3.9\\atari_py\n",
      "  copying atari_py\\__init__.py -> build\\lib.win-amd64-3.9\\atari_py\n",
      "  creating build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\adventure.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\air_raid.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\alien.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\amidar.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\assault.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\asterix.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\asteroids.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\atlantis.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\bank_heist.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\battle_zone.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\beam_rider.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\berzerk.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\bowling.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\boxing.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\breakout.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\carnival.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\centipede.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\chopper_command.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\crazy_climber.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\defender.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\demon_attack.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\double_dunk.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\elevator_action.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\enduro.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\fishing_derby.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\freeway.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\frostbite.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\gopher.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\gravitar.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\hero.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\ice_hockey.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\jamesbond.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\journey_escape.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\kaboom.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\kangaroo.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\krull.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\kung_fu_master.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\montezuma_revenge.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\ms_pacman.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\name_this_game.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\phoenix.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\pitfall.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\pong.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\pooyan.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\private_eye.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\qbert.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\riverraid.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\road_runner.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\robotank.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\seaquest.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\skiing.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\solaris.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\space_invaders.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\star_gunner.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\tennis.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\time_pilot.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\tutankham.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\up_n_down.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\venture.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\video_pinball.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\wizard_of_wor.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\yars_revenge.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  copying atari_py\\atari_roms\\zaxxon.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "  running build_ext\n",
      "  building 'ale_c' extension\n",
      "  creating build\\temp.win-amd64-3.9\n",
      "  creating build\\temp.win-amd64-3.9\\Release\n",
      "  creating build\\temp.win-amd64-3.9\\Release\\atari_py\n",
      "  creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\n",
      "  creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\n",
      "  creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\common\n",
      "  creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\controllers\n",
      "  creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\emucore\n",
      "  creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\emucore\\m6502\n",
      "  creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\emucore\\m6502\\src\n",
      "  creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\environment\n",
      "  creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\games\n",
      "  creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\games\\supported\n",
      "  creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\os_dependent\n",
      "  gcc -mdll -O -Wall -DMS_WIN64 -DBSPF_WIN32 -D_CRT_SECURE_NO_WARNINGS -Iatari_py -Iatari_py\\ale_interface\\src -Iatari_py\\ale_interface\\src\\os_dependent -Iatari_py\\ale_interface\\src\\common -Iatari_py\\ale_interface\\src\\controllers -Iatari_py\\ale_interface\\src\\emucore -Iatari_py\\ale_interface\\src\\emucore\\m6502\\src -Iatari_py\\ale_interface\\src\\emucore\\m6502\\src\\bspf\\src -Iatari_py\\ale_interface\\src\\environment -Iatari_py\\ale_interface\\src\\games -Iatari_py\\ale_interface\\src\\games\\supported -Iatari_py\\ale_interface\\src\\external -Iatari_py\\ale_interface\\src\\external\\TinyMT -IC:\\Users\\acasa\\anaconda3\\include -IC:\\Users\\acasa\\anaconda3\\include -c atari_py\\ale_c_wrapper.cpp -o build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_c_wrapper.o /O2 /GL /GF /EHs-\n",
      "  gcc.exe: error: /O2: No such file or directory\n",
      "  gcc.exe: error: /GL: No such file or directory\n",
      "  gcc.exe: error: /GF: No such file or directory\n",
      "  gcc.exe: error: /EHs-: No such file or directory\n",
      "  error: command 'C:\\\\Users\\\\acasa\\\\anaconda3\\\\Library\\\\mingw-w64\\\\bin\\\\gcc.exe' failed with exit code 1\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for atari-py\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\acasa\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\acasa\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-sparmquk\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\acasa\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-sparmquk\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\acasa\\AppData\\Local\\Temp\\pip-record-hwxy84tk\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\acasa\\anaconda3\\Include\\atari-py'\n",
      "         cwd: C:\\Users\\acasa\\AppData\\Local\\Temp\\pip-req-build-sparmquk\\\n",
      "    Complete output (94 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.9\n",
      "    creating build\\lib.win-amd64-3.9\\atari_py\n",
      "    copying atari_py\\ale_python_interface.py -> build\\lib.win-amd64-3.9\\atari_py\n",
      "    copying atari_py\\__init__.py -> build\\lib.win-amd64-3.9\\atari_py\n",
      "    creating build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\adventure.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\air_raid.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\alien.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\amidar.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\assault.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\asterix.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\asteroids.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\atlantis.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\bank_heist.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\battle_zone.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\beam_rider.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\berzerk.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\bowling.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\boxing.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\breakout.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\carnival.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\centipede.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\chopper_command.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\crazy_climber.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\defender.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\demon_attack.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\double_dunk.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\elevator_action.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\enduro.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\fishing_derby.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\freeway.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\frostbite.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\gopher.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\gravitar.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\hero.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\ice_hockey.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\jamesbond.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\journey_escape.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\kaboom.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\kangaroo.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\krull.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\kung_fu_master.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\montezuma_revenge.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\ms_pacman.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\name_this_game.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\phoenix.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\pitfall.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\pong.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\pooyan.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\private_eye.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\qbert.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\riverraid.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\road_runner.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\robotank.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\seaquest.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\skiing.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\solaris.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\space_invaders.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\star_gunner.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\tennis.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\time_pilot.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\tutankham.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\up_n_down.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\venture.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\video_pinball.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\wizard_of_wor.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\yars_revenge.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    copying atari_py\\atari_roms\\zaxxon.bin -> build\\lib.win-amd64-3.9\\atari_py\\atari_roms\n",
      "    running build_ext\n",
      "    building 'ale_c' extension\n",
      "    creating build\\temp.win-amd64-3.9\n",
      "    creating build\\temp.win-amd64-3.9\\Release\n",
      "    creating build\\temp.win-amd64-3.9\\Release\\atari_py\n",
      "    creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\n",
      "    creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\n",
      "    creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\common\n",
      "    creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\controllers\n",
      "    creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\emucore\n",
      "    creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\emucore\\m6502\n",
      "    creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\emucore\\m6502\\src\n",
      "    creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\environment\n",
      "    creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\games\n",
      "    creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\games\\supported\n",
      "    creating build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_interface\\src\\os_dependent\n",
      "    gcc -mdll -O -Wall -DMS_WIN64 -DBSPF_WIN32 -D_CRT_SECURE_NO_WARNINGS -Iatari_py -Iatari_py\\ale_interface\\src -Iatari_py\\ale_interface\\src\\os_dependent -Iatari_py\\ale_interface\\src\\common -Iatari_py\\ale_interface\\src\\controllers -Iatari_py\\ale_interface\\src\\emucore -Iatari_py\\ale_interface\\src\\emucore\\m6502\\src -Iatari_py\\ale_interface\\src\\emucore\\m6502\\src\\bspf\\src -Iatari_py\\ale_interface\\src\\environment -Iatari_py\\ale_interface\\src\\games -Iatari_py\\ale_interface\\src\\games\\supported -Iatari_py\\ale_interface\\src\\external -Iatari_py\\ale_interface\\src\\external\\TinyMT -IC:\\Users\\acasa\\anaconda3\\include -IC:\\Users\\acasa\\anaconda3\\include -c atari_py\\ale_c_wrapper.cpp -o build\\temp.win-amd64-3.9\\Release\\atari_py\\ale_c_wrapper.o /O2 /GL /GF /EHs-\n",
      "    gcc.exe: error: /O2: No such file or directory\n",
      "    gcc.exe: error: /GL: No such file or directory\n",
      "    gcc.exe: error: /GF: No such file or directory\n",
      "    gcc.exe: error: /EHs-: No such file or directory\n",
      "    error: command 'C:\\\\Users\\\\acasa\\\\anaconda3\\\\Library\\\\mingw-w64\\\\bin\\\\gcc.exe' failed with exit code 1\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\acasa\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\acasa\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-sparmquk\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\acasa\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-sparmquk\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\acasa\\AppData\\Local\\Temp\\pip-record-hwxy84tk\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\acasa\\anaconda3\\Include\\atari-py' Check the logs for full command output.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet==1.5.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: future in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from pyglet==1.5.0) (0.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py==3.1.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from h5py==3.1.0) (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow==9.5.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (9.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-rl2==1.0.5 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from keras-rl2==1.0.5) (2.5.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.37.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.15.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.4.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.19.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.5.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.7.4.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.12)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.11.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.12.1)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.34.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.19.6)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.1.2)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.15.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.14.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.26.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.0.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (58.0.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (6.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Keras==2.2.4 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from Keras==2.2.4) (1.7.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from Keras==2.2.4) (3.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from Keras==2.2.4) (6.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from Keras==2.2.4) (1.19.5)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from Keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from Keras==2.2.4) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from Keras==2.2.4) (1.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.5.3 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (2.5.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (3.3.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (1.19.5)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (1.12)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (3.1.0)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (3.7.4.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (3.19.6)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (1.1.2)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (0.15.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (0.2.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (1.34.1)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (0.37.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (1.12.1)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow==2.5.3) (1.15.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (2.0.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (58.0.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (2.14.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (2.26.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (0.4.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.3) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.3) (6.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.3) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (2023.5.7)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.3) (3.2.2)\n",
      "Requirement already satisfied: torch==2.0.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from torch==2.0.1) (1.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from torch==2.0.1) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from torch==2.0.1) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: filelock in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from torch==2.0.1) (3.3.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from torch==2.0.1) (2.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.1) (1.1.1)\n",
      "\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.1) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: agents==1.4.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: ruamel.yaml in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from agents==1.4.0) (0.17.31)\n",
      "Requirement already satisfied: gym in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from agents==1.4.0) (0.17.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from agents==1.4.0) (2.5.3)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from gym->agents==1.4.0) (1.6.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from gym->agents==1.4.0) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from gym->agents==1.4.0) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from gym->agents==1.4.0) (1.19.5)\n",
      "Requirement already satisfied: future in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym->agents==1.4.0) (0.18.2)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from ruamel.yaml->agents==1.4.0) (0.2.7)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.19.6)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.1.2)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.12)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.6.3)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.11.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.34.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.5.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.7.4.3)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.1.0)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.4.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.12.1)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.15.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.37.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.15.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (58.0.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (0.4.6)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (2.14.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (2.26.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->agents==1.4.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->agents==1.4.0) (6.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (1.26.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\acasa\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.2.2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acasa\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.8\n",
    "else:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, una solución óptima será alcanzada cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_b3mzw8IzJP"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "j3eRhgI-Gb2a"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "#### Configuración base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jwOE6I_KGb2a"
   },
   "outputs": [],
   "source": [
    "# Parametros basicos\n",
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "# Entorno GYM con Videojuego: SpaceInvaders-V0\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Aleatoriedad Reproducible\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "# Informacion sobre el entorno\n",
    "nb_actions = env.action_space.n\n",
    "print(\"El tamaño de nuestro frame es: \", env.observation_space)\n",
    "print(\"El tamaño de la acción es: \", env.action_space.n)\n",
    "print(\"Las acciones disponibles son:\")\n",
    "for action in range(env.action_space.n):\n",
    "    print(\"- Acción\", action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9jGEZUcpGb2a"
   },
   "outputs": [],
   "source": [
    "# Clase para preprocesar: observaciones, lotes de estados y recompensas\n",
    "\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificación del RewardThresholdCallback para parar el entrenamiento del agente cuando se logré una recompensa establecida\n",
    "class RewardThresholdCallback(Callback):\n",
    "    def __init__(self, reward_threshold):\n",
    "        super(RewardThresholdCallback, self).__init__()\n",
    "        self.reward_threshold = reward_threshold\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        if np.mean(logs['episode_reward']) >= self.reward_threshold:\n",
    "            self.model.stop_training = True\n",
    "            print(f\"\\nTraining stopped. Reached reward threshold of {self.reward_threshold}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": [
    "1. Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "O4GKrfWSGb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 605,862\n",
      "Trainable params: 605,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Next, we build our model. We use the same model that was described by Mnih et al. (2015).\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "print(K.image_data_format())\n",
    "if K.image_data_format() == 'channels_last':\n",
    "    # (width, height, channels)\n",
    "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "elif K.image_data_format() == 'channels_first':\n",
    "    # (channels, width, height)\n",
    "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering.')\n",
    "\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "2. Implementación de la solución DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametros\n",
    "\n",
    "# Hiperparametros del modelo\n",
    "tasa_aprendizaje = 0.00025         # Alfa (tasa de aprendizaje)\n",
    "\n",
    "# Parámetros de exploración para la estrategia epsilon-greedy\n",
    "explorar_inicio = 1.0       # Probabilidad de exploración al inicio\n",
    "explorar_fin = 0.1          # Probabilidad mínima de exploración\n",
    "explorar_test = 0.05        # Probabilidad de exploración durante las pruebas\n",
    "total_pasos = 1000000          # Cantidad total de pasos\n",
    "\n",
    "# Hiperparámetros de Q-learning\n",
    "gamma = 0.99                # Tasa de descuento\n",
    "tasa_actualizacion = 10000  # Frecuencia con la que se actualiza el modelo objetivo\n",
    "intervalo_entrenamiento = 4 # Intervalo de entrenamiento en el que se actualiza la red neuronal del agente\n",
    "\n",
    "# Hiperparametros de memoria\n",
    "longitud_preentrenamiento = 50000  # Número de experiencias almacenadas en la memoria al inicializarse por primera vez\n",
    "tam_memoria = 1000000           # Número de experiencias que puede almacenar la memoria\n",
    "\n",
    "# Visualizar el entorno\n",
    "# Si se ejecuta en Colab, se deja en False ya que aún no hemos encontrado la manera de ver la dinamización del juego que no sea en local\n",
    "renderizar_episodio = False\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS CALLBACKS\n",
    "reward_threshold = 20         #Detener el entrenamiento cuando se obtenga 20 de recomepensa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **POLÍTICA DE EXPLORACIÓN**\n",
    "\n",
    "Las siguientes líneas estan relacionadas con la configuración de una política de exploración epsilon-greedy con annealing lineal.\n",
    "\n",
    "- <strong>LinearAnnealedPolicy:</strong> Esta política permite ajustar el valor de epsilon a lo largo del tiempo para equilibrar la exploración y la explotación durante el entrenamiento.\n",
    "\n",
    "- <strong>EpsGreedyQPolicy():</strong> Es una política epsilon-greedy básica que selecciona la acción con la mayor valor Q la mayor parte del tiempo y selecciona una acción aleatoria con una probabilidad epsilon.\n",
    "\n",
    "- <strong>attr='eps':</strong> Especifica que el atributo a ajustar es eps, el cual representa el valor de epsilon.\n",
    "\n",
    "- <strong>value_max=explorar_inicio:</strong> Corresponde al valor inicial de epsilon (el mayor valor de exploración).\n",
    "\n",
    "- <strong>value_min=explorar_fin:</strong> corresponde al valor final de epsilon (el menor valor de exploración).\n",
    "\n",
    "- <strong>value_test=explorar_test: </strong> Corresponde al valor de epsilon a utilizar durante las pruebas o evaluaciones del modelo entrenado.\n",
    "\n",
    "- <strong>nb_steps=total_pasos: </strong>Especifica el número total de pasos de entrenamiento, lo que determina el intervalo de tiempo en el que se realiza el annealing lineal de epsilon.  Que para nuestro caso lo dejamos en 1.000.000\n",
    "\n",
    "Como conclusión en la política implementada, el valor de epsilon se va ajustando progresivamente e inicia desde <strong>explorar inicio (1.0)</strong>, hasta <strong>explorar fin (0.1)</strong> y el total de pasos esta definido por la variable <strong>total pasos (1.000.000)</strong>, lo cual nos permite que el modelo de aprendizaje por refuerzo encuentre un equilibrio entre la exploración inicial y la exploración posterior a medida que el entrenamiento avanza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "foSlxWH1Gb2b"
   },
   "outputs": [],
   "source": [
    "# Agente DQN (Deep Q-Network)\n",
    "\n",
    "# Almacenar y recuperar muestras de experiencia\n",
    "memory = SequentialMemory(limit=tam_memoria, window_length=WINDOW_LENGTH)\n",
    "# Instancia de la clase AtariProcessor\n",
    "processor = AtariProcessor()\n",
    "# Política de exploración\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
    "                              value_max=explorar_inicio, value_min=explorar_fin, value_test=explorar_test,\n",
    "                              nb_steps=total_pasos)\n",
    "# Instancia de la clase DQNAgent\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy,\n",
    "               memory=memory, processor=processor,\n",
    "               nb_steps_warmup=longitud_preentrenamiento, gamma=gamma,\n",
    "               target_model_update=tasa_actualizacion,\n",
    "               train_interval=intervalo_entrenamiento)\n",
    "# Complicion del Agente\n",
    "dqn.compile(Adam(learning_rate=tasa_aprendizaje), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 35s 3ms/step - reward: 0.0124\n",
      "14 episodes - episode_reward: 8.571 [4.000, 16.000] - lives: 2.153 - episode_frame_number: 1071.621 - frame_number: 1441048.421\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 35s 4ms/step - reward: 0.0134\n",
      "16 episodes - episode_reward: 8.312 [3.000, 15.000] - lives: 2.014 - episode_frame_number: 1007.091 - frame_number: 1470937.694\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      " 5258/10000 [==============>...............] - ETA: 16s - reward: 0.0146\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 35s 4ms/step - reward: 0.0140\n",
      "13 episodes - episode_reward: 9.923 [5.000, 24.000] - lives: 2.108 - episode_frame_number: 1260.392 - frame_number: 1500794.846\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 38s 4ms/step - reward: 0.0145\n",
      "14 episodes - episode_reward: 10.929 [6.000, 17.000] - lives: 2.060 - episode_frame_number: 1149.120 - frame_number: 1530752.554\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 34s 3ms/step - reward: 0.0132\n",
      "14 episodes - episode_reward: 9.643 [4.000, 18.000] - lives: 1.995 - episode_frame_number: 1239.279 - frame_number: 1560796.737\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      " 5345/10000 [===============>..............] - ETA: 2:04 - reward: 0.0140\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 269s 27ms/step - reward: 0.0144\n",
      "15 episodes - episode_reward: 9.067 [3.000, 20.000] - loss: 0.013 - mae: 1.070 - mean_q: 1.302 - mean_eps: 0.951 - lives: 2.038 - episode_frame_number: 1045.814 - frame_number: 1590693.112\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0142\n",
      "15 episodes - episode_reward: 10.267 [4.000, 18.000] - loss: 0.011 - mae: 1.106 - mean_q: 1.347 - mean_eps: 0.942 - lives: 2.073 - episode_frame_number: 1206.815 - frame_number: 1620565.420\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      " 2897/10000 [=======>......................] - ETA: 3:18 - reward: 0.0159\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0156\n",
      "14 episodes - episode_reward: 10.571 [4.000, 26.000] - loss: 0.011 - mae: 1.136 - mean_q: 1.387 - mean_eps: 0.933 - lives: 2.098 - episode_frame_number: 1150.144 - frame_number: 1650550.546\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0148\n",
      "16 episodes - episode_reward: 9.438 [4.000, 19.000] - loss: 0.011 - mae: 1.166 - mean_q: 1.422 - mean_eps: 0.924 - lives: 2.096 - episode_frame_number: 1069.554 - frame_number: 1680499.194\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      " 8801/10000 [=========================>....] - ETA: 32s - reward: 0.0155\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0152\n",
      "15 episodes - episode_reward: 9.933 [4.000, 21.000] - loss: 0.011 - mae: 1.196 - mean_q: 1.458 - mean_eps: 0.915 - lives: 2.045 - episode_frame_number: 1098.640 - frame_number: 1710367.946\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 282s 28ms/step - reward: 0.0147\n",
      "16 episodes - episode_reward: 9.562 [5.000, 18.000] - loss: 0.011 - mae: 1.245 - mean_q: 1.517 - mean_eps: 0.906 - lives: 2.152 - episode_frame_number: 1036.632 - frame_number: 1740299.948\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 281s 28ms/step - reward: 0.0131\n",
      "15 episodes - episode_reward: 8.333 [5.000, 15.000] - loss: 0.011 - mae: 1.276 - mean_q: 1.554 - mean_eps: 0.897 - lives: 2.106 - episode_frame_number: 1056.209 - frame_number: 1770203.059\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      " 5141/10000 [==============>...............] - ETA: 2:17 - reward: 0.0148\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 284s 28ms/step - reward: 0.0152\n",
      "15 episodes - episode_reward: 10.000 [3.000, 20.000] - loss: 0.012 - mae: 1.308 - mean_q: 1.593 - mean_eps: 0.888 - lives: 2.013 - episode_frame_number: 1071.718 - frame_number: 1800265.443\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 290s 29ms/step - reward: 0.0140\n",
      "17 episodes - episode_reward: 8.882 [4.000, 14.000] - loss: 0.012 - mae: 1.339 - mean_q: 1.630 - mean_eps: 0.879 - lives: 2.117 - episode_frame_number: 989.008 - frame_number: 1830235.472\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 290s 29ms/step - reward: 0.0152\n",
      "12 episodes - episode_reward: 11.917 [6.000, 19.000] - loss: 0.012 - mae: 1.364 - mean_q: 1.660 - mean_eps: 0.870 - lives: 2.128 - episode_frame_number: 1264.294 - frame_number: 1860109.200\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      " 6149/10000 [=================>............] - ETA: 1:51 - reward: 0.0146\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 8961/10000 [=========================>....] - ETA: 30s - reward: 0.0160\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 294s 29ms/step - reward: 0.0160\n",
      "15 episodes - episode_reward: 10.733 [6.000, 24.000] - loss: 0.011 - mae: 1.370 - mean_q: 1.668 - mean_eps: 0.861 - lives: 2.110 - episode_frame_number: 1101.166 - frame_number: 1889998.401\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      " 7769/10000 [======================>.......] - ETA: 1:07 - reward: 0.0172\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 300s 30ms/step - reward: 0.0160\n",
      "15 episodes - episode_reward: 10.867 [5.000, 25.000] - loss: 0.012 - mae: 1.402 - mean_q: 1.705 - mean_eps: 0.852 - lives: 2.193 - episode_frame_number: 1088.663 - frame_number: 1919989.840\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "  905/10000 [=>............................] - ETA: 4:26 - reward: 0.0232\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 4913/10000 [=============>................] - ETA: 2:31 - reward: 0.0181\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 9709/10000 [============================>.] - ETA: 8s - reward: 0.0175\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 299s 30ms/step - reward: 0.0177\n",
      "13 episodes - episode_reward: 13.462 [6.000, 33.000] - loss: 0.012 - mae: 1.426 - mean_q: 1.734 - mean_eps: 0.843 - lives: 2.235 - episode_frame_number: 1368.108 - frame_number: 1949923.453\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0159\n",
      "15 episodes - episode_reward: 10.400 [4.000, 17.000] - loss: 0.012 - mae: 1.430 - mean_q: 1.742 - mean_eps: 0.834 - lives: 2.172 - episode_frame_number: 1075.896 - frame_number: 1979958.438\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 302s 30ms/step - reward: 0.0146\n",
      "16 episodes - episode_reward: 9.750 [4.000, 19.000] - loss: 0.012 - mae: 1.458 - mean_q: 1.775 - mean_eps: 0.825 - lives: 2.133 - episode_frame_number: 1072.413 - frame_number: 2009881.171\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 303s 30ms/step - reward: 0.0156\n",
      "15 episodes - episode_reward: 9.533 [5.000, 16.000] - loss: 0.014 - mae: 1.508 - mean_q: 1.837 - mean_eps: 0.816 - lives: 2.167 - episode_frame_number: 983.680 - frame_number: 2039875.067\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 307s 31ms/step - reward: 0.0153\n",
      "17 episodes - episode_reward: 9.529 [3.000, 19.000] - loss: 0.013 - mae: 1.535 - mean_q: 1.870 - mean_eps: 0.807 - lives: 2.181 - episode_frame_number: 978.916 - frame_number: 2069814.096\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      " 5273/10000 [==============>...............] - ETA: 2:28 - reward: 0.0174\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0170\n",
      "13 episodes - episode_reward: 12.462 [6.000, 30.000] - loss: 0.013 - mae: 1.572 - mean_q: 1.914 - mean_eps: 0.798 - lives: 2.113 - episode_frame_number: 1267.237 - frame_number: 2099646.532\n",
      "\n",
      "Interval 24 (230000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2509/10000 [======>.......................] - ETA: 3:54 - reward: 0.0187\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 6273/10000 [=================>............] - ETA: 1:57 - reward: 0.0179\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 7825/10000 [======================>.......] - ETA: 1:08 - reward: 0.0181\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 315s 31ms/step - reward: 0.0178\n",
      "14 episodes - episode_reward: 12.857 [6.000, 25.000] - loss: 0.014 - mae: 1.615 - mean_q: 1.965 - mean_eps: 0.789 - lives: 2.150 - episode_frame_number: 1182.856 - frame_number: 2129581.999\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      " 3661/10000 [=========>....................] - ETA: 3:18 - reward: 0.0186\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0168\n",
      "15 episodes - episode_reward: 11.400 [6.000, 24.000] - loss: 0.014 - mae: 1.677 - mean_q: 2.038 - mean_eps: 0.780 - lives: 2.106 - episode_frame_number: 1085.343 - frame_number: 2159609.473\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0142\n",
      "15 episodes - episode_reward: 8.467 [5.000, 14.000] - loss: 0.014 - mae: 1.680 - mean_q: 2.043 - mean_eps: 0.771 - lives: 2.209 - episode_frame_number: 1021.797 - frame_number: 2189543.425\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "    9/10000 [..............................] - ETA: 5:10 - reward: 0.0000e+00\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 7273/10000 [====================>.........] - ETA: 1:27 - reward: 0.0172\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0170\n",
      "15 episodes - episode_reward: 12.533 [6.000, 26.000] - loss: 0.014 - mae: 1.701 - mean_q: 2.068 - mean_eps: 0.762 - lives: 2.214 - episode_frame_number: 1105.734 - frame_number: 2219483.367\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      " 3097/10000 [========>.....................] - ETA: 3:46 - reward: 0.0171\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 331s 33ms/step - reward: 0.0169\n",
      "13 episodes - episode_reward: 12.769 [8.000, 20.000] - loss: 0.014 - mae: 1.716 - mean_q: 2.086 - mean_eps: 0.753 - lives: 2.107 - episode_frame_number: 1185.029 - frame_number: 2249399.679\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 329s 33ms/step - reward: 0.0167\n",
      "15 episodes - episode_reward: 11.067 [6.000, 17.000] - loss: 0.015 - mae: 1.745 - mean_q: 2.122 - mean_eps: 0.744 - lives: 2.116 - episode_frame_number: 1127.488 - frame_number: 2279312.093\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 332s 33ms/step - reward: 0.0161\n",
      "15 episodes - episode_reward: 10.600 [4.000, 17.000] - loss: 0.014 - mae: 1.781 - mean_q: 2.165 - mean_eps: 0.735 - lives: 2.049 - episode_frame_number: 1063.654 - frame_number: 2309213.007\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      " 1105/10000 [==>...........................] - ETA: 4:57 - reward: 0.0226\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 3789/10000 [==========>...................] - ETA: 3:25 - reward: 0.0195\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 5989/10000 [================>.............] - ETA: 2:13 - reward: 0.0189\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 335s 33ms/step - reward: 0.0169\n",
      "15 episodes - episode_reward: 11.933 [2.000, 28.000] - loss: 0.014 - mae: 1.822 - mean_q: 2.215 - mean_eps: 0.726 - lives: 2.044 - episode_frame_number: 1223.297 - frame_number: 2339164.645\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 342s 34ms/step - reward: 0.0166\n",
      "16 episodes - episode_reward: 10.000 [5.000, 16.000] - loss: 0.015 - mae: 1.834 - mean_q: 2.229 - mean_eps: 0.717 - lives: 2.154 - episode_frame_number: 955.071 - frame_number: 2369060.229\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      " 3885/10000 [==========>...................] - ETA: 3:28 - reward: 0.0190\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 364s 36ms/step - reward: 0.0178\n",
      "16 episodes - episode_reward: 11.062 [2.000, 24.000] - loss: 0.015 - mae: 1.852 - mean_q: 2.249 - mean_eps: 0.708 - lives: 2.046 - episode_frame_number: 998.153 - frame_number: 2398899.054\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 358s 36ms/step - reward: 0.0160\n",
      "15 episodes - episode_reward: 10.600 [5.000, 18.000] - loss: 0.014 - mae: 1.876 - mean_q: 2.279 - mean_eps: 0.699 - lives: 2.137 - episode_frame_number: 1124.628 - frame_number: 2428691.604\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      " 2533/10000 [======>.......................] - ETA: 4:31 - reward: 0.0186\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 365s 36ms/step - reward: 0.0174\n",
      "15 episodes - episode_reward: 12.133 [4.000, 28.000] - loss: 0.015 - mae: 1.899 - mean_q: 2.306 - mean_eps: 0.690 - lives: 2.089 - episode_frame_number: 1164.381 - frame_number: 2458584.938\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      " 5601/10000 [===============>..............] - ETA: 2:44 - reward: 0.0182\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 375s 37ms/step - reward: 0.0176\n",
      "15 episodes - episode_reward: 11.267 [6.000, 23.000] - loss: 0.017 - mae: 1.939 - mean_q: 2.355 - mean_eps: 0.681 - lives: 2.138 - episode_frame_number: 1032.231 - frame_number: 2488485.557\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 377s 38ms/step - reward: 0.0185\n",
      "15 episodes - episode_reward: 12.533 [5.000, 19.000] - loss: 0.015 - mae: 1.964 - mean_q: 2.386 - mean_eps: 0.672 - lives: 2.071 - episode_frame_number: 1120.052 - frame_number: 2518432.607\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      " 5721/10000 [================>.............] - ETA: 2:45 - reward: 0.0184\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 8753/10000 [=========================>....] - ETA: 48s - reward: 0.0182\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 9697/10000 [============================>.] - ETA: 11s - reward: 0.0185\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 392s 39ms/step - reward: 0.0187\n",
      "13 episodes - episode_reward: 14.077 [4.000, 26.000] - loss: 0.015 - mae: 2.023 - mean_q: 2.456 - mean_eps: 0.663 - lives: 2.195 - episode_frame_number: 1324.972 - frame_number: 2548391.284\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      " 1277/10000 [==>...........................] - ETA: 5:42 - reward: 0.0180\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 4085/10000 [===========>..................] - ETA: 3:47 - reward: 0.0186\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 376s 38ms/step - reward: 0.0186\n",
      "12 episodes - episode_reward: 14.833 [10.000, 20.000] - loss: 0.017 - mae: 2.066 - mean_q: 2.506 - mean_eps: 0.654 - lives: 2.023 - episode_frame_number: 1275.667 - frame_number: 2578466.017\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      " 7285/10000 [====================>.........] - ETA: 1:45 - reward: 0.0189\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 390s 39ms/step - reward: 0.0184\n",
      "14 episodes - episode_reward: 14.143 [8.000, 20.000] - loss: 0.016 - mae: 2.079 - mean_q: 2.524 - mean_eps: 0.645 - lives: 2.061 - episode_frame_number: 1196.575 - frame_number: 2608408.285\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      " 2581/10000 [======>.......................] - ETA: 4:55 - reward: 0.0186\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 8937/10000 [=========================>....] - ETA: 43s - reward: 0.0175\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 415s 41ms/step - reward: 0.0175\n",
      "12 episodes - episode_reward: 14.750 [6.000, 22.000] - loss: 0.017 - mae: 2.104 - mean_q: 2.552 - mean_eps: 0.636 - lives: 2.009 - episode_frame_number: 1348.141 - frame_number: 2638336.050\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      " 2033/10000 [=====>........................] - ETA: 5:44 - reward: 0.0187\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 438s 44ms/step - reward: 0.0183\n",
      "16 episodes - episode_reward: 11.438 [4.000, 20.000] - loss: 0.017 - mae: 2.146 - mean_q: 2.601 - mean_eps: 0.627 - lives: 2.090 - episode_frame_number: 1006.758 - frame_number: 2668297.115\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      " 1533/10000 [===>..........................] - ETA: 6:14 - reward: 0.0189\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 2561/10000 [======>.......................] - ETA: 5:29 - reward: 0.0191\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 3541/10000 [=========>....................] - ETA: 4:46 - reward: 0.0198\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 7637/10000 [=====================>........] - ETA: 1:44 - reward: 0.0191\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 443s 44ms/step - reward: 0.0188\n",
      "12 episodes - episode_reward: 15.167 [8.000, 29.000] - loss: 0.018 - mae: 2.175 - mean_q: 2.635 - mean_eps: 0.618 - lives: 1.971 - episode_frame_number: 1353.250 - frame_number: 2698252.884\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      " 1457/10000 [===>..........................] - ETA: 6:22 - reward: 0.0192\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 6745/10000 [===================>..........] - ETA: 2:26 - reward: 0.0194\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 471s 47ms/step - reward: 0.0184\n",
      "14 episodes - episode_reward: 13.000 [6.000, 23.000] - loss: 0.017 - mae: 2.215 - mean_q: 2.685 - mean_eps: 0.609 - lives: 2.119 - episode_frame_number: 1146.324 - frame_number: 2728221.544\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      " 1117/10000 [==>...........................] - ETA: 7:57 - reward: 0.0197\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 496s 50ms/step - reward: 0.0173\n",
      "16 episodes - episode_reward: 10.812 [7.000, 21.000] - loss: 0.018 - mae: 2.248 - mean_q: 2.724 - mean_eps: 0.600 - lives: 2.221 - episode_frame_number: 987.829 - frame_number: 2758220.220\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      " 1265/10000 [==>...........................] - ETA: 6:40 - reward: 0.0198\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 4297/10000 [===========>..................] - ETA: 4:19 - reward: 0.0189\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 453s 45ms/step - reward: 0.0181\n",
      "13 episodes - episode_reward: 13.538 [7.000, 23.000] - loss: 0.018 - mae: 2.261 - mean_q: 2.739 - mean_eps: 0.591 - lives: 2.069 - episode_frame_number: 1235.321 - frame_number: 2788225.368\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      " 2061/10000 [=====>........................] - ETA: 6:05 - reward: 0.0180\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 472s 47ms/step - reward: 0.0170\n",
      "12 episodes - episode_reward: 14.167 [6.000, 21.000] - loss: 0.019 - mae: 2.283 - mean_q: 2.763 - mean_eps: 0.582 - lives: 2.184 - episode_frame_number: 1321.482 - frame_number: 2818238.912\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "  501/10000 [>.............................] - ETA: 7:30 - reward: 0.0200\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 2813/10000 [=======>......................] - ETA: 5:51 - reward: 0.0178\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 4721/10000 [=============>................] - ETA: 4:16 - reward: 0.0193\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 7237/10000 [====================>.........] - ETA: 2:14 - reward: 0.0189\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 9261/10000 [==========================>...] - ETA: 35s - reward: 0.0184\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 484s 48ms/step - reward: 0.0181\n",
      "12 episodes - episode_reward: 15.250 [5.000, 27.000] - loss: 0.020 - mae: 2.329 - mean_q: 2.819 - mean_eps: 0.573 - lives: 2.088 - episode_frame_number: 1355.559 - frame_number: 2848135.119\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      " 2421/10000 [======>.......................] - ETA: 6:12 - reward: 0.0182\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 493s 49ms/step - reward: 0.0175\n",
      "14 episodes - episode_reward: 12.643 [6.000, 21.000] - loss: 0.018 - mae: 2.365 - mean_q: 2.863 - mean_eps: 0.564 - lives: 2.194 - episode_frame_number: 1181.954 - frame_number: 2878071.714\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 499s 50ms/step - reward: 0.0167\n",
      "15 episodes - episode_reward: 11.600 [6.000, 19.000] - loss: 0.019 - mae: 2.417 - mean_q: 2.926 - mean_eps: 0.555 - lives: 2.090 - episode_frame_number: 1133.091 - frame_number: 2907993.755\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      " 1249/10000 [==>...........................] - ETA: 7:30 - reward: 0.0264\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 3289/10000 [========>.....................] - ETA: 5:42 - reward: 0.0231\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 4353/10000 [============>.................] - ETA: 4:47 - reward: 0.0223\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 507s 51ms/step - reward: 0.0199\n",
      "15 episodes - episode_reward: 13.200 [3.000, 25.000] - loss: 0.019 - mae: 2.459 - mean_q: 2.977 - mean_eps: 0.546 - lives: 2.195 - episode_frame_number: 1079.795 - frame_number: 2938019.247\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      " 3249/10000 [========>.....................] - ETA: 5:45 - reward: 0.0188\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 4889/10000 [=============>................] - ETA: 4:23 - reward: 0.0200\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 5725/10000 [================>.............] - ETA: 3:40 - reward: 0.0208\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 7353/10000 [=====================>........] - ETA: 2:16 - reward: 0.0209\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 513s 51ms/step - reward: 0.0203\n",
      "12 episodes - episode_reward: 15.917 [8.000, 25.000] - loss: 0.019 - mae: 2.476 - mean_q: 2.997 - mean_eps: 0.537 - lives: 2.081 - episode_frame_number: 1290.692 - frame_number: 2967941.936\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      " 2705/10000 [=======>......................] - ETA: 6:20 - reward: 0.0200\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 5565/10000 [===============>..............] - ETA: 3:48 - reward: 0.0198\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 507s 51ms/step - reward: 0.0192\n",
      "14 episodes - episode_reward: 14.143 [7.000, 30.000] - loss: 0.019 - mae: 2.483 - mean_q: 3.004 - mean_eps: 0.528 - lives: 2.093 - episode_frame_number: 1199.286 - frame_number: 2997836.958\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      " 9149/10000 [==========================>...] - ETA: 44s - reward: 0.0200\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 527s 53ms/step - reward: 0.0201\n",
      "14 episodes - episode_reward: 14.643 [8.000, 20.000] - loss: 0.020 - mae: 2.515 - mean_q: 3.043 - mean_eps: 0.519 - lives: 2.161 - episode_frame_number: 1178.010 - frame_number: 3027795.589\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "  753/10000 [=>............................] - ETA: 8:18 - reward: 0.0226\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 1769/10000 [====>.........................] - ETA: 7:21 - reward: 0.0220\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 5161/10000 [==============>...............] - ETA: 4:21 - reward: 0.0198\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 8033/10000 [=======================>......] - ETA: 1:46 - reward: 0.0200\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 541s 54ms/step - reward: 0.0197\n",
      "14 episodes - episode_reward: 14.357 [6.000, 22.000] - loss: 0.021 - mae: 2.559 - mean_q: 3.097 - mean_eps: 0.510 - lives: 2.167 - episode_frame_number: 1189.678 - frame_number: 3057673.753\n",
      "\n",
      "Interval 56 (550000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1797/10000 [====>.........................] - ETA: 7:20 - reward: 0.0206\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 2793/10000 [=======>......................] - ETA: 6:30 - reward: 0.0211\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 5021/10000 [==============>...............] - ETA: 4:31 - reward: 0.0227\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 5997/10000 [================>.............] - ETA: 3:38 - reward: 0.0228\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 7329/10000 [====================>.........] - ETA: 2:26 - reward: 0.0225\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 9101/10000 [==========================>...] - ETA: 49s - reward: 0.0215\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 9977/10000 [============================>.] - ETA: 1s - reward: 0.0216\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 549s 55ms/step - reward: 0.0216\n",
      "11 episodes - episode_reward: 19.727 [9.000, 28.000] - loss: 0.020 - mae: 2.556 - mean_q: 3.093 - mean_eps: 0.501 - lives: 2.235 - episode_frame_number: 1445.091 - frame_number: 3087649.446\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "  873/10000 [=>............................] - ETA: 8:39 - reward: 0.0229\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 5121/10000 [==============>...............] - ETA: 4:33 - reward: 0.0203- ETA: 4:34 - reward: \n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 557s 56ms/step - reward: 0.0194\n",
      "13 episodes - episode_reward: 14.538 [5.000, 23.000] - loss: 0.020 - mae: 2.645 - mean_q: 3.200 - mean_eps: 0.492 - lives: 2.064 - episode_frame_number: 1182.481 - frame_number: 3117600.909\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      " 4245/10000 [===========>..................] - ETA: 5:14 - reward: 0.0198\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 6505/10000 [==================>...........] - ETA: 3:14 - reward: 0.0206\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 568s 57ms/step - reward: 0.0199\n",
      "14 episodes - episode_reward: 13.643 [6.000, 22.000] - loss: 0.021 - mae: 2.659 - mean_q: 3.215 - mean_eps: 0.483 - lives: 2.146 - episode_frame_number: 1134.086 - frame_number: 3147460.252\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      " 3937/10000 [==========>...................] - ETA: 5:51 - reward: 0.0178\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 7581/10000 [=====================>........] - ETA: 2:20 - reward: 0.0194\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 574s 57ms/step - reward: 0.0192\n",
      "14 episodes - episode_reward: 14.643 [7.000, 23.000] - loss: 0.021 - mae: 2.673 - mean_q: 3.231 - mean_eps: 0.474 - lives: 2.112 - episode_frame_number: 1218.497 - frame_number: 3177410.005\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      " 1101/10000 [==>...........................] - ETA: 8:24 - reward: 0.0182\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 2093/10000 [=====>........................] - ETA: 7:21 - reward: 0.0201\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 8417/10000 [========================>.....] - ETA: 1:31 - reward: 0.0195\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 9933/10000 [============================>.] - ETA: 3s - reward: 0.0195\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 587s 59ms/step - reward: 0.0194\n",
      "13 episodes - episode_reward: 14.923 [7.000, 23.000] - loss: 0.022 - mae: 2.700 - mean_q: 3.266 - mean_eps: 0.465 - lives: 2.260 - episode_frame_number: 1269.503 - frame_number: 3207356.510\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      " 5617/10000 [===============>..............] - ETA: 4:32 - reward: 0.0208\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 6461/10000 [==================>...........] - ETA: 3:39 - reward: 0.0214\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 617s 62ms/step - reward: 0.0211\n",
      "15 episodes - episode_reward: 13.600 [6.000, 21.000] - loss: 0.022 - mae: 2.696 - mean_q: 3.260 - mean_eps: 0.456 - lives: 2.157 - episode_frame_number: 1035.776 - frame_number: 3237435.567\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      " 6521/10000 [==================>...........] - ETA: 3:29 - reward: 0.0204\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 599s 60ms/step - reward: 0.0196\n",
      "12 episodes - episode_reward: 15.167 [11.000, 21.000] - loss: 0.022 - mae: 2.711 - mean_q: 3.277 - mean_eps: 0.447 - lives: 2.101 - episode_frame_number: 1270.075 - frame_number: 3267459.237\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "  237/10000 [..............................] - ETA: 9:50 - reward: 0.0127\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 626s 63ms/step - reward: 0.0187\n",
      "13 episodes - episode_reward: 15.538 [9.000, 24.000] - loss: 0.021 - mae: 2.727 - mean_q: 3.297 - mean_eps: 0.438 - lives: 2.107 - episode_frame_number: 1273.585 - frame_number: 3297371.689\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      " 1325/10000 [==>...........................] - ETA: 9:00 - reward: 0.0234\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 2429/10000 [======>.......................] - ETA: 7:55 - reward: 0.0214\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 4865/10000 [=============>................] - ETA: 5:23 - reward: 0.0193\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 629s 63ms/step - reward: 0.0178\n",
      "12 episodes - episode_reward: 14.833 [5.000, 23.000] - loss: 0.022 - mae: 2.748 - mean_q: 3.322 - mean_eps: 0.429 - lives: 2.104 - episode_frame_number: 1407.775 - frame_number: 3327207.533\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      " 4161/10000 [===========>..................] - ETA: 6:13 - reward: 0.0173\n",
      "Training stopped. Reached reward threshold of 20.\n",
      " 5313/10000 [==============>...............] - ETA: 4:58 - reward: 0.0186\n",
      "Training stopped. Reached reward threshold of 20.\n",
      "10000/10000 [==============================] - 629s 63ms/step - reward: 0.0196\n",
      "14 episodes - episode_reward: 13.714 [7.000, 27.000] - loss: 0.020 - mae: 2.741 - mean_q: 3.313 - mean_eps: 0.420 - lives: 2.072 - episode_frame_number: 1177.722 - frame_number: 3357142.245\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      " 1585/10000 [===>..........................] - ETA: 8:41 - reward: 0.0170done, took 24650.872 seconds\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del Agente\n",
    "\n",
    "# Guardar los pesos del modelo entrenado\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "# Callbacks para controlar el entrenamiento del agente\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
    "callbacks += [FileLogger(log_filename, interval=100)]\n",
    "callbacks += [RewardThresholdCallback(reward_threshold=reward_threshold)]\n",
    "\n",
    "# Ejecucion del entrenamiento\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=1000000, log_interval=10000, visualize=renderizar_episodio)\n",
    "# Guardar pesos finales del entrenamiento\n",
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OHYryKd1Gb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 28.000, steps: 1366\n",
      "Episode 2: reward: 22.000, steps: 976\n",
      "Episode 3: reward: 10.000, steps: 623\n",
      "Episode 4: reward: 8.000, steps: 610\n",
      "Episode 5: reward: 15.000, steps: 969\n",
      "Episode 6: reward: 9.000, steps: 476\n",
      "Episode 7: reward: 4.000, steps: 358\n",
      "Episode 8: reward: 8.000, steps: 513\n",
      "Episode 9: reward: 18.000, steps: 939\n",
      "Episode 10: reward: 13.000, steps: 662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19e6f074760>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prueba\n",
    "# Archivo de pesos obtenidos en el entrenamiento\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "# Cargar los pesos\n",
    "dqn.load_weights(weights_filename)\n",
    "# Generar Test\n",
    "dqn.test(env, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 19.000, steps: 882\n",
      "Episode 2: reward: 18.000, steps: 930\n",
      "Episode 3: reward: 13.000, steps: 712\n",
      "Episode 4: reward: 8.000, steps: 480\n",
      "Episode 5: reward: 13.000, steps: 934\n",
      "Episode 6: reward: 15.000, steps: 768\n",
      "Episode 7: reward: 12.000, steps: 641\n",
      "Episode 8: reward: 9.000, steps: 722\n",
      "Episode 9: reward: 7.000, steps: 478\n",
      "Episode 10: reward: 16.000, steps: 919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19fc0d336d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Segundo Test\n",
    "dqn.test(env, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo con los resultados obtenidos, se evidencia que el en un primer entrenamiento de 1.750.000 y luego en otro de 1.000.000 con algunos pocos cambios, pasos logró alcanzar una recompensa acumulada mayor de 20 en varios episodios durante el entrenamiento. Esto, permite concluir que el agente fue capaz de aprender y mejorar su desempeño a lo largo del proceso de entrenamiento, basado en la arquitectura de CNN implementada y en la configuración de los hiperparametros. Los resultados obtenidos se pueden atribuir a los siguientes factores:\n",
    "\n",
    "Estrategias de exploración y explotación:\n",
    "\n",
    "El uso de una política epsilon-greedy con una tasa de exploración inicial alta y una tasa mínima de exploración permitió al agente explorar el entorno y descubrir acciones que conducían a mayores recompensas. En la medida que la tasa de exploración disminuía permitió al agente aprovechar más el conocimiento adquirido y tomar decisiones óptimas (explotación).\n",
    "\n",
    "Capacidad de generalización:\n",
    "\n",
    "El modelo de red neuronal utilizado basado en el estado de arte es muy utilizado en varios trabajos y proyectos en los cuales se evidencia buenos resultados (ver referencias). Esta arquitectura permitió al agente generalizar su conocimiento y aplicarlo a nuevas instancias del entorno, lo que condujo a un mejor rendimiento.\n",
    "\n",
    "Aprendizaje de políticas óptimas:\n",
    "\n",
    "El algoritmo DQN utilizado es capaz de aprender las políticas óptimas basadas en el valor de acción Q, lo cual se evidencia durante el entrenamiento, en el cual se observa que el agente pudo ajustar continuamente sus estimaciones de valor Q a medida que se encontraba con nuevas experiencias, lo que le permitió aprender y mejorar sus decisiones en función de las recompensas esperadas.\n",
    "\n",
    "En cuanto al uso de memoria de repetición secuencial permitió al agente almacenar y recordar experiencias pasadas, lo que facilitó un entrenamiento más eficiente y un mejor aprendizaje.\n",
    "\n",
    "En este ultimo factor, es de gran importancia para el proceso de entrenamiento el cual por ser mediante iteraciones y con el objetivo de obtener una recompensa acumulada mayor de 20 tanto en local como en colab se presentaba algunas limitaciones o como sucedió el entrenamiento tardo un tiempo considerable, algo común en los proyectos de referencia. \n",
    "\n",
    "Otra conclusión fue que era necesario tener un callback que detuviera el entrenamiento cuando se alcanzaba una recompensa determinada y no se avanzaba en los entrenamientos siguientes. Es por eso que obtuvimos un mejor rendimiento en el entrenamiento de un millón de pasos vs el entrenamiento de 1.7 millones de pasos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANFQiicXK3sO"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
